<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Neo"><title>打造分布式文件系统-Mapreduce · 浮生若梦</title><meta name="description" content="前面进行了Hadoop配置安装
以及了解了HDFS
接下来就是Mapreduce了.其概念就是Map（映射）Reduce（化简）主要思想，都是从函数式编程语言里借来的，以及从矢量编程语言里借来的特性.


相关的原理咱先忽略,直接进入实例应用.
从hadoop源码中hadoop-2.2.0-src/"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title><a href="/">浮生若梦</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>打造分布式文件系统-Mapreduce</a></h3></div><div class="post-content"><p>前面进行了<a href="http://www.floatinglife.cn/hadoop-install" target="_blank" rel="noopener">Hadoop配置安装</a></p>
<p>以及了解了<a href="http://www.floatinglife.cn/hadoop-hdfs" target="_blank" rel="noopener">HDFS</a></p>
<p>接下来就是Mapreduce了.其概念就是Map（映射）Reduce（化简）主要思想，都是从函数式编程语言里借来的，以及从矢量编程语言里借来的特性.</p>
<a id="more"></a>

<p>相关的原理咱先忽略,直接进入实例应用.</p>
<p>从hadoop源码中hadoop-2.2.0-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/<br>打开其示例程序WordCount如下:</p>
<pre><code>public class WordCount {

  public static class TokenizerMapper 
       extends Mapper&lt;Object, Text, Text, IntWritable&gt;{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer 
       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable values, 
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    if (otherArgs.length != 2) {
      System.err.println(&quot;Usage: wordcount  &quot;);
      System.exit(2);
    }
    Job job = new Job(conf, &quot;word count&quot;);
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}</code></pre><p>分为三步:</p>
<ol>
<li>TokenizerMapper 类实现 Mapper 接口中的 map 方法，输入参数中的 value<br>是文本文件中的一行</li>
<li>IntSumReducer类实现 Reducer 接口中的 reduce 方法, 输入参数中的 key,<br>values 是由 Map 任务输出的中间结果，values 是一个 Iterator, 遍历这个<br>Iterator, 就可以得到属于同一个 key 的所有 value. 此处，key<br>是一个单词，value 是词频。只需要将所有的 value<br>相加，就可以得到这个单词的总的出现次数。</li>
<li>配置 Job并运行</li>
</ol>
<p>就这么简单的三步,实现了分布式统计一批文本文件中单词出现的频率的功能.</p>
<p><a name="table1"></a><strong>JobConf 常用可定制参数</strong></p>
<table summary="JobConf 常用可定制参数" width="100%" border="0" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<th scope="col">
参数

</th>
<th scope="col">
作用

</th>
<th scope="col">
缺省值

</th>
<th scope="col">
其它实现

</th>
</tr>
<tr>
<th scope="row">
**InputFormat**

</th>
<td>
将输入的数据集切割成小数据集 InputSplits, 每一个 InputSplit 将由一个
Mapper 负责处理。此外 InputFormat 中还提供一个 RecordReader 的实现,
将一个 InputSplit 解析成 <key,value\> 对提供给 map 函数。

</key,value\></td>
<td>
TextInputFormat  
(针对文本文件，按行将文本文件切割成 InputSplits, 并用 LineRecordReader
将 InputSplit 解析成 <key,value\> 对，key 是行在文件中的位置，value
是文件中的一行)

</key,value\></td>
<td>
SequenceFileInputFormat

</td>
</tr>
<tr>
<th scope="row">
**OutputFormat**

</th>
<td>
提供一个 RecordWriter 的实现，负责输出最终结果

</td>
<td>
TextOutputFormat  
(用 LineRecordWriter 将最终结果写成纯文件文件,每个 <key,value\>
对一行，key 和 value 之间用 tab 分隔)

</key,value\></td>
<td>
SequenceFileOutputFormat

</td>
</tr>
<tr>
<th scope="row">
**OutputKeyClass**

</th>
<td>
输出的最终结果中 key 的类型

</td>
<td>
LongWritable

</td>
<td>
</td>
</tr>
<tr>
<th scope="row">
**OutputValueClass**

</th>
<td>
输出的最终结果中 value 的类型

</td>
<td>
Text

</td>
<td>
</td>
</tr>
<tr>
<th scope="row">
**MapperClass**

</th>
<td>
Mapper 类，实现 map 函数，完成输入的 <key,value\> 到中间结果的映射

</key,value\></td>
<td>
IdentityMapper  
(将输入的 <key,value\> 原封不动的输出为中间结果)

</key,value\></td>
<td>
LongSumReducer,  
LogRegexMapper,  
InverseMapper

</td>
</tr>
<tr>
<th scope="row">
**CombinerClass**

</th>
<td>
实现 combine 函数，将中间结果中的重复 key 做合并

</td>
<td>
null  
(不对中间结果中的重复 key 做合并)

</td>
<td>
</td>
</tr>
<tr>
<th scope="row">
**ReducerClass**

</th>
<td>
Reducer 类，实现 reduce 函数，对中间结果做合并，形成最终结果

</td>
<td>
IdentityReducer  
(将中间结果直接输出为最终结果)

</td>
<td>
AccumulatingReducer, LongSumReducer

</td>
</tr>
<tr>
<th scope="row">
**InputPath**

</th>
<td>
设定 job 的输入目录, job 运行时会处理输入目录下的所有文件

</td>
<td>
null

</td>
<td>
</td>
</tr>
<tr>
<th scope="row">
**OutputPath**

</th>
<td>
设定 job 的输出目录，job 的最终结果会写入输出目录下

</td>
<td>
null

</td>
<td>
</td>
</tr>
<tr>
<th scope="row">
**MapOutputKeyClass**

</th>
<td>
设定 map 函数输出的中间结果中 key 的类型

</td>
<td>
如果用户没有设定的话，使用 OutputKeyClass

</td>
<td>
</td>
</tr>
<tr>
<th scope="row">
**MapOutputValueClass**

</th>
<td>
设定 map 函数输出的中间结果中 value 的类型

</td>
<td>
如果用户没有设定的话，使用 OutputValuesClass

</td>
<td>
</td>
</tr>
<tr>
<th scope="row">
**OutputKeyComparator**

</th>
<td>
对结果中的 key 进行排序时的使用的比较器

</td>
<td>
WritableComparable

</td>
<td>
</td>
</tr>
<tr>
<th scope="row">
**PartitionerClass**

</th>
<td>
对中间结果的 key 排序后，用此 Partition 函数将其划分为R份,每份由一个
Reducer 负责处理。

</td>
<td>
HashPartitioner  
(使用 Hash 函数做 partition)

</td>
<td>
KeyFieldBasedPartitioner PipesPartitioner

</td>
</tr>
</tbody>
</table>

<p>咱们以后再研究其内部实现及原理</p>
<hr>
<p>参考资料:</p>
<p><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop2/" target="_blank" rel="noopener">http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop2/</a></p>
<p><a href="http://www.infoq.com/cn/articles/MapReduce-Best-Practice-1" target="_blank" rel="noopener">http://www.infoq.com/cn/articles/MapReduce-Best-Practice-1</a></p>
<p><a href="http://hadoop.apache.org/docs/r0.19.1/cn/mapred_tutorial.html#%E7%9B%AE%E7%9A%84" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r0.19.1/cn/mapred_tutorial.html#%E7%9B%AE%E7%9A%84</a></p>
<p><a href="http://www.cnblogs.com/xia520pi/archive/2012/06/04/2534533.html" target="_blank" rel="noopener">http://www.cnblogs.com/xia520pi/archive/2012/06/04/2534533.html</a></p>
<p><a href="http://blog.csdn.net/kauu/article/details/1815353" target="_blank" rel="noopener">http://blog.csdn.net/kauu/article/details/1815353</a></p>
<p><a href="http://sishuok.com/forum/blogPost/list/0/5965.html" target="_blank" rel="noopener">http://sishuok.com/forum/blogPost/list/0/5965.html</a></p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2013-11-01</span><i class="fa fa-tag"></i><a class="tag" href="/categories/技术流/" title="技术流">技术流 </a><a class="tag" href="/tags/hadoop/" title="hadoop">hadoop </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,http://yoursite.com/2013/11/01/hadoop-mapreduce/,浮生若梦,打造分布式文件系统-Mapreduce,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2013/11/01/mongodb-shell-options/" title="mongodb  shell 操作">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2013/10/31/what-if-facebook-of-the-dead/" title="【What if 系列】Facebook之死亡国">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>